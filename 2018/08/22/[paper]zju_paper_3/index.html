<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          [Notes]  跨语言实体对齐模型 - Aerber Joy
        
    </title>

    <link rel="canonical" href="https://mengyingzhou.github.io/2018/08/22/[paper]zju_paper_3/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('bg.png')
            /*post*/
        
    }
    
    #signature{
        background-image: url('/img/signature/BeanTechSign-white.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Paper" title="Paper">Paper</a>
                            
                              <a class="tag" href="/tags/#Internship" title="Internship">Internship</a>
                            
                              <a class="tag" href="/tags/#Deep Learning" title="Deep Learning">Deep Learning</a>
                            
                              <a class="tag" href="/tags/#Knowledge Graph" title="Knowledge Graph">Knowledge Graph</a>
                            
                              <a class="tag" href="/tags/#Attention Based Model" title="Attention Based Model">Attention Based Model</a>
                            
                        </div>
                        <h1>[Notes]  跨语言实体对齐模型</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Aerber Zhou on
                            2018-08-22
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Aerber Joy</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="outline">Outline</h1>
<p>在浙江大学实习的课题是“跨语言的实体融合”，但是最后只做了对齐，还没有涉及到融合的方面。此文主要记录一下在实习阶段自己做的一些model、result还有future work。</p>
<p>主要分为以下几大板块：</p>
<ol>
<li>Introduction</li>
<li>Model Details</li>
<li>Related Work</li>
<li>Future Work</li>
<li>Reference</li>
</ol>
<h1 id="introduction">Introduction</h1>
<h2 id="background">Background</h2>
<p>在知识图谱中有三大任务，其中有一个就是<strong>共指消解</strong>[1]，指的是将不同形式，但是是指向同一项实体的表示对齐。我们的课题“跨语言的实体对齐”就是其中一种，是对齐用不同语言指代的同一种实体，例如“apple phone”和“苹果手机”其实指的是同一样东西。对齐在知识融合中具有很大的基础价值，一旦实体对齐之后，才能很好的将两种或者多种不同语言、媒体的知识进行相互的补充和融合。</p>
<h2 id="data">Data</h2>
<p>数据来自<strong>浙江大学DCD实验室 鲁伟明教授小组</strong>。</p>
<h2 id="aims">Aims</h2>
<p>对于一个中文百度百科实体，在100个候选Wikipedia中找出其对应的相同指代实体。</p>
<p>我们采用多种深度学习模型，输出计算后的候选者分数，分数越高说明越有可能是和中文对应的实体。我们设立了top1（前1名准确率）和top10（前10位准确率）最为衡量模型优劣的标准。</p>
<h1 id="model-details">Model Details</h1>
<p>在浙大的时候我尝试了2种模型，一种是<strong>BiLSTM+CNN</strong>，一种是<strong>Attention+BiLSTM+CNN</strong>。前者效果比较好，最高top1是17%，top10是64.4%。</p>
<h2 id="bilstmcnn">BiLSTM+CNN</h2>
<h3 id="model">Model</h3>
<p><img src="model1.png" alt="model1.png"><br>
模型整体比较简单。需要说明的是，训练的时候采用的是三元组的训练方式，为（中文实体，英文正例实体，英文负例实体）。</p>
<p><strong>先讲一下训练模型：</strong></p>
<ol>
<li>
<p>embedding layer</p>
<p>将百度百科和两个Wikipedia的摘要转换为embedding，得到三个[embedding size, sequence length, 1]的矩阵。</p>
</li>
<li>
<p>bi-lstm layer</p>
<p>将前面一层得到的embedding数据，依次喂入到Bi-LSTM中。Bi-LSTM的数量为sequence length。</p>
<p>此层最终保留Bi-LSTM的hidden outputs，输出三个[sequence length, lstm hidden size * 2]的摘要表达。</p>
</li>
<li>
<p>concat layer</p>
<p>将前一层得到的中文实体摘要表示和英文正例、负例摘要表示分别在第二维上进行连接。</p>
<p>即concat（[hiddens_zh, hiddens_pos]），concat（[hiddens_zh, hiddens_neg]），得到两个[sequence length, lstm hidden size * 4]中英文交互表达。</p>
<p>为了输入到CNN中，在扩充了最后一维，表示1通道。</p>
<p>即此层最终输出两个[sequence length, lstm hidden size * 4，1]向量。</p>
</li>
<li>
<p>cnn layer</p>
<p>将上一层的输出，通过一层或者多层CNN+max pooling（通过cnn_layer_size_list参数动态调整）得到输出n个cnn output，输出大小根据参数变化。</p>
<p>在实验的过程，我采用3层cnn，kernel size分别为[3,4,5]时效果最好。</p>
</li>
<li>
<p>full connection layer</p>
<p>首先将上一层得到的n层CNN输出全部concat，并且reshape成一个一维的矩阵。</p>
<p>在这个一维矩阵上采用带L2正则化的full connection，分别得到postive的分数和negative的分数。</p>
</li>
<li>
<p>loss + optimizator</p>
<p>设定magrin，计算loss和accuracy。并采用AdamOptimizer。</p>
</li>
</ol>
<p><strong>预测模型</strong></p>
<p>同训练模型类似，只是在预测模型中，我们是将需要预测的中文实体和100个候选者依次以（中文实体，正例实体）的方式结对放入模型中，而不放入负例。</p>
<p>通过整个模型之后，输出100个候选者的分数，计算top1和top10的准确率。</p>
<h3 id="model-result">Model Result</h3>
<table>
<thead>
<tr>
<th>hyper parameter</th>
<th>Train accuracy</th>
<th>Val accuracy</th>
<th>Test accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Filter number =16，Barch size =256</td>
<td>96.8%</td>
<td>Top1：21.7% Top10：65.8%</td>
<td>Top1：13.5% Top10：60%</td>
</tr>
<tr>
<td>Filter number =128，Barch size =256</td>
<td>100%</td>
<td>Top1：21.7% Top10：71.3%</td>
<td>Top1：17% Top10：64.4%</td>
</tr>
<tr>
<td>Filter number =64，Barch size =128</td>
<td>98.4%</td>
<td>Top1：14.7% Top10：65.1%</td>
<td>Top1：14.4% Top10：60.2%</td>
</tr>
<tr>
<td>Filter number =64，Barch size =32</td>
<td>100%</td>
<td>Top1：13.9% Top10：46.5%</td>
<td>Top1：9.25% Top10：48.2%</td>
</tr>
</tbody>
</table>
<h2 id="attentionbilstmcnn">Attention+BiLSTM+CNN</h2>
<p>在第一个模型的基础上，在参考了[2]的attention机制之后，在embedding layer和bi-LSTM layer之间加了一层attention。如下图:<br>
<img src="model2.png" alt="model2"></p>
<p>其中attention的细节图如下两张：<br>
<img src="2-detail1.png" alt="2-detail1"></p>
<p>下图是每一个attention的内部结构图:<br>
<img src="2-detail2.png" alt="2-detail2"></p>
<h3 id="model-result">Model Result</h3>
<p>修改后的模型效果并没有改进，在Filter number =128，Barch size =256 的条件下，测试集上最好的结果也只是top1：12%，top10: 46%。</p>
<h1 id="related-work">Related Work</h1>
<p><strong>陈璐 @ 中山大学</strong> 复现了 <strong>Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search</strong>，并应用到此课题上。</p>
<p><strong>郭悦 @ 中山大学</strong> 复现了 <strong>Bilateral Multi-Perspective Matching for Natural Language Sentences</strong>。同时基于此论文，将最后一层LSTM换成CNN后，将准确率提高了8%。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Train accuracy</th>
<th>Val accuracy</th>
<th>Test accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conv-K-NRM</td>
<td>100%</td>
<td>Top1：35% Top10：85%</td>
<td>Top1：29% Top10：73.8%</td>
</tr>
<tr>
<td>Bi-MPM+CNN</td>
<td>100%</td>
<td>Top1：25.7% Top10：72.0%</td>
<td>Top1：27% Top10：72.9%</td>
</tr>
</tbody>
</table>
<h1 id="future-work">Future Work</h1>
<p>在同组内其他同学的对比之后，觉得自己的模型的准确率有点低。因此考虑一些改进措施。</p>
<h2 id="图片attention做补充">图片Attention做补充</h2>
<p><img src="Image_Caption_Generation.png" alt="Image_Caption_Generation"></p>
<p>上图是文章[3]中提出的基于attention的图片字幕生成模型。下图是模型在某一个测试图片上的可视化表示：</p>
<p><img src="visual1.png" alt="visual1"><br>
<img src="visual2.png" alt="visual2"><br>
借助文章中对于图片采用attention mechanism生成字幕的方法，我们可以利用此方法提取某一个中文百科对应的多张图片的比较general的图片描述。再利用这种图片描述，对摘要或者标题进行一个attention的辅助，或者直接只用图片做对齐。</p>
<p>下图是当此图片描述作为其他模型的attention的辅助的模型描述：</p>
<p><img src="pic_attention_aid.png" alt="pic_attention_aid"></p>
<p>模型解释：</p>
<ol>
<li>
<p>首先将某一个百科或者Wikipedia的图片通过论文中提出的模型（即图中上半部分的模型）得到某一张百科/Wikipedia的图片描述。</p>
<p>然后将百科和postive，百科和negative分别在第三维上concat，变成2通道的矩阵。</p>
<p>在2通道数据的基础上，通过一层或多层CNN得到结对交互表示。</p>
</li>
<li>
<p>取出我前面提出的BiLSTM+CNN中的CNN layer后的输出表示为X，将得到的（百科，postive），（百科，negative）的结对表示，在数据X上做attention mechanism。</p>
</li>
<li>
<p>最后将attention mechanism的输出通过一层或多层全连接得到最终分数。</p>
</li>
</ol>
<p>模型弊端：</p>
<ol>
<li>
<p>此方法的弊端一，文章[3]模型的方法是否具有较高的普适性，对于我们百度百科/Wikipedia的图片数据，是不是会出现比较大的噪声。</p>
</li>
<li>
<p>第二个问题，因为我曾经在实验室的服务器上跑过<a href="https://github.com/yunjey/show-attend-and-tell" target="_blank" rel="noopener">此论文的复现</a>，但是最终以oom的报错没有跑出结果。所以可能如果需要尝试的话，可能需要升级服务器配置。</p>
</li>
<li>
<p>第三个问题就是在<a href="https://github.com/yunjey/show-attend-and-tell" target="_blank" rel="noopener">论文的复现</a>中，最终输出是one hot编码，如果用word embedding来训练会比较好，这样相当于是从图片向量空间转换到文字向量空间中，这样方便去补充到过去一些通过NLP手段做对齐的模型中。</p>
</li>
</ol>
<h2 id="多粒度表示">多粒度表示</h2>
<p>在文章[4]的启发下，考虑到能否采用多粒度的表征进行学习。即不仅有word embedding，还有sentence embedding，甚至可以context embedding去做分层化的学习。</p>
<p><img src="HAN.png" alt="HAN"></p>
<p>这样的话，模型会提供层次结构, 反映文件中word，sentence两个粒度的层次结构。</p>
<p>同时是在多个粒度上分别做Attention Mechanism, 使其能够在构建文档表示时，对不重要和重要的内容进行差异化表示。</p>
<p><strong>此方法的可行度较高，但是数据预处理那一块可能需要做一个简单修改。</strong></p>
<h2 id="kcnnknowledge-aware-convolutional-neural-network">KCNN（knowledge-aware convolutional neural network）</h2>
<p>虽然这篇文章[5]的方向是news recommendation，不是知识图谱，但是在论文中他利用KG去对news recommendation做补充而提出的KCNN（knowledge-aware convolutional neural network）却有借鉴意义。</p>
<p>全文的推荐模型如下：</p>
<p><img src="DKN.png" alt="DKN.png"></p>
<p>我们主要关注的是KCNN这一块，其他的部分都是比较常规的模型搭建方法。其他部分的讲解可以看<a href="https://qiufengyu.github.io/2018/04/17/reading14/" target="_blank" rel="noopener">这个详细说明</a></p>
<p>KCNN模型同样也体现出了多粒度这么一个概念，从word embedding到entity embedding，再到context embedding，但是不同于文章[4]中提出的entity embedding是由word embedding得出，context embedding是由entity embedding得出这种机制，在这篇论文中，这三个粒度之间的交互并不强。</p>
<p><strong>三种粒度的表示：</strong></p>
<p>word embedding是普通的生成方式。</p>
<p>而entity embedding是由下图这种图的方式进行构建：提取相关文本中的实体，构建知识网络，再降维到二维空间，得到每一个涉及到的实体的向量表达。</p>
<p><img src="DKN_2.png" alt="DKN_2.png"></p>
<p>context embedding则是由下图方式得到：也是提取实体，构建知识网络，并按照一定的依据划分出context embedding。</p>
<p><img src="DKN_3.png" alt="DKN_3.png"></p>
<p><strong>此方法的个人想法</strong></p>
<p>这个方法比较创新，因为本来此论文就是今年1月份发表的。</p>
<p>但是要从此入手的话，比较费时费力，因为从前的在数据集上的工作的可能并不能做一个很好的重用，而且entity embedding和context embedding的构建也比较麻烦。</p>
<p>如果能和上交或者微软研究所那边的老师进行联系，得到相关数据的支持，可能会方便一点。</p>
<h1 id="reference">Reference</h1>
<ol>
<li>刘峤, 李杨, 段宏,等. 知识图谱构建技术综述[J]. 计算机研究与发展, 2016, 53(3):582-600.</li>
<li>Yang Z, Yang D, Dyer C, et al. Hierarchical Attention Networks for Document Classification[C]// Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2017:1480-1489.</li>
<li>Xu K, Ba J, Kiros R, et al. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention[J]. Computer Science, 2015:2048-2057.</li>
<li>Yang Z, Yang D, Dyer C, et al. Hierarchical Attention Networks for Document Classification[C]// Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2017:1480-1489.</li>
<li>Wang H, Zhang F, Xie X, et al. DKN: Deep Knowledge-Aware Network for News Recommendation[J]. 2018.</li>
</ol>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2018/10/20/[233]gym/" data-toggle="tooltip" data-placement="top" title="[233] 我要减肥～">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2018/08/18/[paper]zju_paper_2/" data-toggle="tooltip" data-placement="top" title="[Paper Notes]  02-浙江大学实习期间看的一些论文和写的一些代码">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
				<div id="vcomments"></div>
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#outline"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Outline</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#introduction"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Introduction</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#background"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">Background</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#data"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">Data</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#aims"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">Aims</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#model-details"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Model Details</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#bilstmcnn"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">BiLSTM+CNN</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#model"><span class="toc-nav-number">3.1.1.</span> <span class="toc-nav-text">Model</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#model-result"><span class="toc-nav-number">3.1.2.</span> <span class="toc-nav-text">Model Result</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#attentionbilstmcnn"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">Attention+BiLSTM+CNN</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#model-result"><span class="toc-nav-number">3.2.1.</span> <span class="toc-nav-text">Model Result</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#related-work"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Related Work</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#future-work"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Future Work</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#图片attention做补充"><span class="toc-nav-number">5.1.</span> <span class="toc-nav-text">图片Attention做补充</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#多粒度表示"><span class="toc-nav-number">5.2.</span> <span class="toc-nav-text">多粒度表示</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#kcnnknowledge-aware-convolutional-neural-network"><span class="toc-nav-number">5.3.</span> <span class="toc-nav-text">KCNN（knowledge-aware convolutional neural network）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#reference"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">Reference</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Paper" title="Paper">Paper</a>
                        
                          <a class="tag" href="/tags/#Internship" title="Internship">Internship</a>
                        
                          <a class="tag" href="/tags/#Deep Learning" title="Deep Learning">Deep Learning</a>
                        
                          <a class="tag" href="/tags/#Knowledge Graph" title="Knowledge Graph">Knowledge Graph</a>
                        
                          <a class="tag" href="/tags/#Attention Based Model" title="Attention Based Model">Attention Based Model</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="undefined" target="_blank">Mobile Systems and Networking Group @ Fudan University</a></li>
                    
                        <li><a href="https://chenyang03.wordpress.com/" target="_blank">Prof. Yang Chen</a></li>
                    
                        <li><a href="http://oss.lzu.edu.cn" target="_blank">LZU OSS</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- disqus embedded js code start (one page only need to embed once) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "your-disqus-ID";
    var disqus_identifier = "https://mengyingzhou.github.io/2018/08/22/[paper]zju_paper_3/";
    var disqus_url = "https://mengyingzhou.github.io/2018/08/22/[paper]zju_paper_3/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus embedded js code start end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>

    <script>
        new Valine({
            el: '#vcomments',
            appId: '7TGrkWBjg6DPEdjFcbXOaWbN-gzGzoHsz',
            appKey: 's9n02dNw41r6yqXsoJzvaEEz',
			placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
			visitor: true
        })
    </script>	
<script type="text/javascript" src="/js/zooming.js"></script>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                    <li>
                        <a target="_blank" href="https://twitter.com/myzhou97">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                

                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/aerber.zhou.9">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://github.com/mengyingzhou">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://www.linkedin.com/in/aerber-zhou-600bb4175">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Aerber Zhou 2020 
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a> 
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span> 
                    re-Ported by <a href="https://github.com/NullAerber">NullAerber</a> | 
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=NullAerber&repo=NullAerber.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://mengyingzhou.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://mengyingzhou.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
